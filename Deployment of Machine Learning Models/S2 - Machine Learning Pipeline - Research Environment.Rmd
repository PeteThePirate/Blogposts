---
title: "Deploying Machine Learning Models"
author: "Peter Smith"
date: "08/07/2020"
output: 
  html_document:
    toc: true
    toc_float:
      toc_collapsed: true
---

# Section 2: ML Pipeline - Research Environment

![*Machine Learning Pipeline - circled modules need to be put into production*](./S2 - Machine Learning Pipeling.PNG)

# Section 3: ML System Architecture

Machine learning in production requires multiple different components in order to work. 'Architecture' describes the way software components are arranged and the interactions between them.


### Specific Challenges of ML Systems

* Need for reproducibility
* Entanglement - changing 1 input feature could change all weights. Changing anything changes everything
* Data dependencies - Traditional software only has a code dependency. We have a data dependency as well, and data inputs can be unstable
* Configuration issues - not clear, but seems to relate to hyperparameter decisions perhaps 
* Data and feature preparation - systems can have massive amounts of supporting code to prepare data (think TFG online model)
* Model errors can be hard to detect with traditional tests
* Separation of expertise - because deploying a model requires so many people, its possible everyone just blindly does their role and no one understands the entire process.

![*Challenges in research vs production environments*](./S2 - Research vs Production.PNG)

### Key Principles for ML System Architecture

* Reproducibility: Have the ability to replicate a given ML prediction
* Automation: Retrain, update, deploy models as part of an automated pipeline
* Extensibility: Have the ability to easily add and update models
* Modularity: Preprocessing/feature engineering code used in training should be organised into clear pipelines
* Scalability: Ability to serve model predictions to large numbers of customers
* Testing: Test variation between model versions

### General ML Architectures

1. Train by batch, predict on the fly, serve via REST API (focus for this course)
2. Train by batch, predict by batch, serve via shared database (TFG Online)
3. Train, predict by streaming
4. Train by batch, predict on mobile (or other client)

![Architecture Comparison] (./S2 - ML Architectures.PNG)

### Architecture breakdown for train by batch, predict on the fly

The steps our system architecture needs to take can be broken up into multiple layers:

1. __Data Layer__: provide access to all data sources
2. __Feature Layer__: generate features in transparent, reusable and scalable way
3. __Scoring Layer__: transform features into predictions
4. __Evaluation Layer__: checks equivalence of 2 models - eg actual vs predicted

![*Note overlap of code on feature extraction between training and production*](./S3 - Predict on the Fly diagram.PNG)

### Building a Reproducible ML Pipeline

We need to guarantee the entire pipeline is reproducible so that a model and its predictions can be replicated exactly. This requires reproducibility in the following steps.

__Input data__

* Save a snapshot of training data if possible (may break data practice rules). Otherwise design data sources with accurate timestamps so that a view of the data at a point in time can be retrieved

* SQL may return results in different order - it may be important to keep track of training vs testing sets used.

__Feature Engineering__

* Code to generate features should be tracked properly with version control and published with auto-incremented or timestamp hashed versions (not sure I understand how to auto-increment to track which version of a function is used).
* Sample means used for normalisation/imputation should be stored.
* If randomly imputing, remember to set a seed.

__Model Building__

* Record the order of features
* Record feature transformations (log, standardisation, etc)
* Record hyperparameters
* Set seed if model has a random element
* if the final model is an ensemble, record the ensemble structure

__Model Deployment__

* Software versions should match exactly - list libraries and their versions.
* Use a container and track its specifications (such as image version)
* Research, develop and deploy utilising the same language. (Don't build in R, deploy in Python)
* Make sure model integration is understood before deployment and develop with this is mind
  - avoid using features not available when the model is live
  - watch out for live-population being different from training-population.
  
# Section 4: Pipeline - Writing Production Code

### Overview

We need to write production code for the 3 steps circled in red in section 2. Namely, Preprocession, Variable Selection, and Model Building. We typically need to move from Notebooks into Scripts.

Deployment code can be written in 3 main ways:

* __Procedural Programming__: Write a sequence of functions as in Jupyter

The other ways use Object-Oriented Programming

* __Custom Pipeline__: Calls procedures in order
* __Third-party Pipeline__: eg. Sci-kit learn pipeline

### Procedural Programming

Procedural programming involves writing procedures/functions/etc that can be called in a specific order. This is what I have done so far. Good start where you are the sole data scientist not supported by many developers, and there are not many models created. However where models are constantly deployed, updated and refreshed it is not a good solution.

Typically involves the following:

* A yaml file containing user-defined constants (paths to data or models, column-means used for normalisation, features required in data)
* Script containing processing function definitions
* Control script that is executed and runs all functions in the correct order

__Advantages__:

* Straightforward to create after notebooks
* No software development skills required
* Easy to manually check if it reproduces the original model

__Disadvantages__:

* Can get buggy
* Difficult to test (mostly manual)
* Difficult to build software on top of it
* Need to save a lot of intermediate files to store the transformation parameters

### Custom Pipeline

A pipeline is a set of dataprocessing steps connected in series.

We write our code in the form of objects. Objects can store data, as well as methods to modify that data. Thus objects can learn and store parameters for transformations as attributes.

* Parameters are automatically refreshed every time a model is retrained
* 2 methods will typically be created
  + Fit: to learn parameters and save parameter as an object attribute
  + Transform: To transform input data with the learnt parameters

The sequence in which to pass data through objects is then saved as a pipeline.

The entire pipeline is then saved as a single object - a Python pickle - with all information required for transforming and scoring data to get a prediction. Thus you have a single object that is your entire model - from data transformation through to scoring.


__Advantages__:

* Can be tested, versioned, tracked and controlled
* Can build future models on top
* Good software developer practice
* Built to satisfy business needs

__Disadvantages__:

* Requires team of software developers to build and maintain
* Overhead for data scientist to familiarise with code for debugging or adding on future models
* Preprocessor not reusable, need to rewrite preprocessor class for each new ML model
* Need to write new pipeline for each new ML model
* Lacks versatility - may constrain the data scientist to what is available with an implemented pipeline



