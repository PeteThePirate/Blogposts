---
title: "Neural Network Notes"
---

The purpose of this notebook is for me to document a whole bunch of little tips and tricks for myself that I may want to refer to later when building Feed Forward Neural Nets. It's largely based on things I've picked up from Andrew Ng's Deep Learning Specialization on Coursera, but over time I'll hopefully be updating it with a things I've learnt from other sources, and learnings from mistakes I make along the way.

Because I'm writing this primarily for quick reference in future, this notebook tends to have a much drier tone than I would like. Sorry about that...
 
# Activation Functions

I'm not going to describe each function here since that's really easy to look up. Rather I'm just going to make some notes about usage.

* Tanh is a superior choice to sigmoid function for hidden layers since it is exactly equal to the sigmoid function but shifted to be centered at zero.

* Sigmoid function is useful as an output layer since it ranges between 0 and 1 (it's basically logistic regression)

* __ReLU should be my default activation function for hidden layers__.

* Softmax function is used as a final layer if we are using multiclass classification. It standardises the outputs to be interpretable as probabilities by ensuring they sum to 1. 

# Train/Dev/Test set selection

For smaller datasets, 70/30/0 or 60/20/20 splits were common

But in practice, the Dev set only needs to be big enough to choose between multiple algorithms, and the test set only needs to be big enough to get a good estimate of the performance of the model on out-of-training data. This need scales with *n* rather than by *p*, so for a datasets of say 1,000,000 rows, it may be reasonable to take 10,000 rows each for dev and test and using the remainder for training.

Don't rob from your training data unnecessarily.

# Dealing with Bias/Variance or overfitting/underfitting

Bias/Variance is an alternative view of "underfitting/overfitting" on your training set and we diagnose it by looking at the performance of the model on the training set versus the performance of the model on the dev set.

With Neural Nets, there is less of a tradeoff in the minimising bias and minimising variance. This is because we have tools that can isolate improvement in one metric while having little effect on the other. For example, if we have high bias (regardless of the variance), we could fit a bigger network (more layers or neurons in a layers) or train the neural net for longer. Once we are satisfied with the bias, we can deal with variance by either getting more data, or introducing more regularisation.
With regards to regularisation, it basically means training a bigger neural net never hurts, it just takes longer.

So the tradeoff is no longer really *bias vs variance*, but rather *bias vs time to train*.

# Regularisation


## L2-regularisation (weight-decay)
As expected from other regularisations, for example in lasso regression, regularisation in Neural Networks works by adding a penalisation term onto your cost function.

If we are to regularise the L2-norm (which would usually be the case), we can express the cost function to be minimised  as
$J(w,b) = \frac{1}{m} \sum_{\forall m} \mathcal{L}(\hat y^{(i)},y^{(i)}) + \frac{\lambda}{2m}||w||^2_2$.

Andrew Ng recommends not bothering to regularise the bias term as well simply because the dimensionality of $w \in \mathcal{R}^{n_x}$ makes it much more important than $b \in \mathcal{R}$. But the recommendation seems to be 'its not worth the effort of coding it up' rather than 'there is some advantage to leaving it out'. The recommendation is made in the video [Improving Deep Neural Networks: Hyperparameter tuning, Regularisation and Optimization/Week 1/Regularisation](https://www.coursera.org/learn/deep-neural-network/lecture/Srsrc/regularisation).

For terminology reference, note that $||w||^2_2 = \sum_{\forall{l}}{||w^{[l]}||^2}$ where $||w^{[l]}||^2$ is the square of the *Frobenius* norm.

The term *weight decay* is because in backpropogation, the weight update step becomes $w_{new}^{[l]} =  \left( 1-  \frac{\alpha\lambda}{m} \right) w_{old}^{[l]}  - \alpha [\text{normal backprop value}]$.

__Why does L2-regularisation work?__

Ng gives intuition about setting Neurons to zero so fewer Neurons are used. I don't believe this is correct. Rather, because of how L2 works, its more efficient for the Network to have many small activations than a few large activations. This should make the model need signal from a variety of sources to have large values, rather than just 1 neural path lighting up.

There is also intuition that keeping weights small, you also keep the inputs to the activation small, and so over a small range, the *tanh* and *sigmoid* functions are more approximately linear.

__L1 Regularisation__

Using L1-regularisation can also be done, with the effect that it will make your neural network more sparse by setting lots of the weights to zero. However in the same video as linked above, the comment is made that the compression gains aren't great, so there should be some other motivating reason to use L1 over L2.

## Dropout Regularisation

__What is it?__

Works by randomly selecting some Neurons, and setting their activations to zero. Different Neurons are selected in each training epoch.
*Inverted Dropout* is the most common method of implementing dropout. Inverted Dropout reweights the incoming activations for each neuron in a layer by the expected number of live-neurons in the previous layer. This makes sure the overall magnitude of the activations of each layer remains constant through training and implementation.
For example, if 80% of neurons in layer 1 are kept, you would take divide each activation in layer 1 by 0.8 when calculating the input to the activations in layer 2. 


__Why does Dropout work?__

At Jupytercon 2017, it was pointed out that a Neural Network with connections missing is kind of like a Random Forest, but with some connections between some of the trees. So maybe it's mimicing ensemble learning with a few networks in some sense, although with some violation of the usual independence assumption of bagging. During training it is also as if each of these networks being trained is smaller, so it mimics having less complicated networks.

Each neuron in the network can also not rely too much on any 1 neuron in the previous layer (since it might disappear), so it should learn signal from each source instead. This is a similar effect to L2-regularisation.

__Notes on Dropout__

* Usually we don't apply dropout to the first layer of the network (ie, the actual features from the training data).
* Dropout will add jitter to the Cost-graph, so it will not be monotonically decreasing. 


## Other Regularisation methods
* Get more training data by augmenting training set (flipping, rotating, random cropping etc)
* Early stopping - stop updating the Neural Network when the dev-set error starts increasing. This does have a downside though in that you're entering into the bias/variance tradeoff. Other methods are *Orthogonal* in that they can fix one aspect of your model while having little effect on others.

# Setting up your optimization problem

## Normalize your inputs
As usual, its best to normalise data to be zero-mean and unit variance. Note - calculate the training set means and variances for each feature, and use these means and variances on the test set and in production as well. It will make your network train faster.

## Vanishing/Exploding Gradients
Imagine a very deep network, such as in the image provided.

![example NN for exploding/vanishing gradients from Andrew Ng's Coursera course](Images/Vanishing_gradients_example.png)

For simplicity, if we assume a linear activation, and no bias terms, then $\hat y = w^{[L]}w^{[L-1]}\dots w^{[3]}w^{[2]}w^{[1]}X$. If the average value of each of the weight matrices is less than 1, we can see that the value of y will tend towards 0. Similarly if the average magnitude is greater than 1, the value of y will explode. Furthermore, the activations will all also explode or vanish, since $a^{[l]} = w^{[l]}w^{[l-1]}\dots w^{[1]}X$. By 'average value of each of the weight matrices', I think this might just mean the determinants of each of these matrices, but I'd need to check that. 

The Vanishing Gradient problem can be solved by using a better choice of Weight Initialisation for the network (See Xavier weight initialisation section).

##Weight Initialization

To solve for the Vanishing/Exploding Gradient problem, we can control the magnitudes of the weight-matrices when we initialise them to have roughly unit-magnitude. We do this by scaling the variance of each individual element within the matrix so the the sum of the variances is 1. 

So if a neuron has $n$ inputs, we can initialise each weight as a random-normal variable with variance proportional to $1/n$.

* For ReLU, it works best to have variance $2/n$. I guess because only half the x-axis is actually used in ReLU?
* tanh, it works best to have variance $1/n$ (This is Xavier initialisation)

This initialisation magnitude can be tuned, but it's super low priority.

#Optimisation Algorithms

The key takeouts here are:

* Always use mini-batch learning (unless the input data is super small). 
* Always use Adam optimisation. Adam includes momentum as well as RMSProp, so it seems to be the good default choice. 

##Mini-batch gradient descent

Basically, it's a waste of time to use all the millions of training points to adjust the weights by a super small learning_rate. Rather use really small subsets of the data, to get a rough estimate of the gradient, then perform backpropagation. You may jitter around a bit, but you'll generally be going in the right direction. And using Momentum or Adam we can fix this jitter anyway. 

The steps for mini-batch are:

* Shuffle your data
* Segment your data into small subsets (called a mini-batch). Andrew Ng suggests much smaller than I expected. 
* For each mini-batch, perform an iteration of forward and backward propogation
* go to the next mini-batch

__Some notes on mini-batch__

* Going through your entire dataset once is referred to as an *'epoch'*.
* Your cost function will no longer be strictly decreasing as you iterate, since you may randomly get harder-to-predict mini-batches.
*Typically mini-batches are of size 64,128,256,512. 
*If your dataset is small, don't bother (say, fewer than 2000 samples).
*Make sure your mini-batch fits its CPU/GPU memory, otherwise you'll have a lot of loss in learning time.

##Exponentially Weighted Averages

##Gradient Descent with Momentum

##RMSProp

To be honest I didn't really get RMSProp

##Adam Optimisation

Basically Adam is RMSProp + Momentum. Use this as your default optimiser.

##Learning Rate Decay

##Problem of Local Optima

#Hyperparameter Tuning

##Order of hyperparameter importance for tuning

This is based on Andrew Ng's recommendations

1. The learning rate
  
2.
    a. Momentum (default ~ 0.9)
    b. The number of hidden units
    c. Mini-batch size

3. 
    a. The number of layers
    b. Learning rate decay
  

4. Basically only ever use default parameters for Adam Algorithm
    a. $\beta_1 = 0.9$
    b. $\beta_2 = 0.99$
    c. $\epsilon = 10^{-8}$

##Approaches to hyperparameter tuning

Sometimes, data-scientists will babysit a single model, manually tuning and refining it so the the model is constantly improving over time. 

Other times, a data-scientist will train many models in parallel and see which model performs the best after a decent amount of time. 

Andrew Ng refers to this as the Panda approach vs the Caviar approach - where pandas have a single child that they care for, whereas fish lay many eggs, and wait and see which ones survive. 

To choose between these approaches, you must consider how much computational resource you have. If you have plenty, use the Caviar approach. If you are more constrained, use the Panda approach. 

## Don't sample in a grid. Rather sample at random

![from Andrew Ng's Coursera course](Images/random_hyperparameters.png)

Imagine you have 2 hyperparameters to tune, but hyperparameter 2 actually has no effect on the performance of your model. If you sample 25 points in a $5 \times 5$ grid, you've only tested 5 distinct values for hyperparameter 1. But if you'd randomly selected points in the 2-dimensional space, even if hyperparameter 2 is useless, you've still tested 25 distinct values for hyperparameter 1.

##Course-to-fine scheme

Instead of sampling uniformly over the entire space of candidate hyperparameter values, you can first select a few random values over a large space, then observe which regions seem to have the best performance. A second stage of searching can then be performed only in this region where we have seen the model performs well.

##Sample on the appropriate scale (log-sampling for some parameters)

Say you're tuning the learning rate, and you suspect the best value lies in the region $(0.0001, 1)$. If you sample uniformly, you will be using roughly 99% of your samples to test values between 0.001 and 1, and only 1% of your sample to test between 0.0001 and 0.001. It makes more sense to randomly select values on the log-scale using the uniform distribution, and then to convert those values back to the linear scale.

So for the example given above one would

1. Generate r = -4U where U ~ Uniform(0,1). Then $r \in [-4,0]$

2. Set $\alpha = 10^r$ so that $\alpha \in [10^{-4}, 10^0]$ or $\alpha \in [0.0001,1]$

Similarly, say we want to use exponentially weighted averages, and we want to test values for $\beta \in [0.9, 0.999]$. The process would be

1. Generate r = -2U - 1. Then $r \in [-3,-1]$
2. Set $1-\beta = 10^r$. Then $\beta = 1-10^r \in [0.9, 0.999]$ as required.

##Batch-Normalization

This makes your Neural Network much more robust to hyperparameter selection and thus speeds up training.

Instead of just normalising the inputs to the Neural Network, we normalise the inputs every layer by normalizing either the activations, or the linear-component of the previous layer. It is debatable which should be normalised, but the most common is to normalise the linear component. 

__To perform batch normalisation__

* For each $Z^{(i)}$ in layer l, set $Z_{norm}^{(i)} = \frac{ Z^{(i)} - \mu}{\sqrt{\sigma^2 + \epsilon}}$ using the (mini-batch) sample mean and variance. 
* Then set $\tilde{z} = \gamma Z_{norm}^{(i)} + \beta$ where $\gamma$ and $\beta$ are learnable parameters of the model.

Basically, we are setting the location and distribution of each neuron to be learnable parameters.

__Using batch-norm replaces the bias parameter in each node__
This is because $\beta$ and the bias parameter would both act as scaling-constants. It's maybe not clear in the example above, but $\gamma$ and $\beta$ would both be vectors with dimensions $(n^{[l]}, 1)$ (ie, every neuron gets it's own learned value).

__Why does batch-norm work?__

There are multiple intuitions here:

1. In the same way that normalising the inputs to the first layer of the network speeds up learning, normalising later layers should do the same.

2. During training, a deeper layer is learning the best weights based on some values of activations of the previous layer. But these previous layer values are also getting updated, so there is 'covariate shift'. By using $z_{norm}$, then applying our own scaling, we 'undo' any shifts in the activations, and the neuron can focus on learning from the patterns of its inputs instead.

3. Has a slight regularization effect. Since when training with mini-batches, we normalise with a different sample mean and variance each time, we are introducing some noise into the activations of the hidden layer. This is sort of the same as dropout though which adds news to activations by randomly removing nodes. Thus adding regularization forces the model to not depend too much on any one feature. This is kind of flimsy though, and Batch-norm shouldn't be used for the intention of regularization.

__At prediction time__

At prediction on a single sample, you won't have a sample mean and variance to use to calculate $z_{norm}$. To deal with this, at training time, we keep track of a $\mu$ and $\sigma$ estimate using exponentially weighted moving averages during training. These values are then used during prediction. Most frameworks should take this into account for you so I wouldn't worry about it. 




