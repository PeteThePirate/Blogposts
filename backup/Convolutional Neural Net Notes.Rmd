---
title: "Convolutional Neural Nets"
author: "Peter Smith"
date: "13 May 2020"
output:
  html_document:
    toc: yes
    toc_collapsed: yes
    toc_float: yes
    theme: darkly
---

```{r}
library(hwriter)
library(knitr)
library(dplyr)
library(kableExtra)
```


##Convolutions explained using edge-detection

A convolution is a mathematical operation on two functions that productes a third function expressing how the shape of one is modified by the other. Similar to my honours project, a convolution on an image is created by applying some *kernel*, or synonymously *filter*, to every spot on a matrix to produce some resultant matrix

![Example convolution for detecting vertical edges](Images/Convolution_Example_Vertical_edge.PNG)

So a vertical edge detector filter may involve a kernel

$$
\begin{pmatrix}
1& 0 & -1\\ 
1& 0 & -1\\
1& 0 & -1\\
\end{pmatrix}.
$$
However is this the best kernel for a vertical edge detector? Alternatives recommended in literature are the Sobel and Scharr filters given  by


$$
\begin{pmatrix}
1& 0 & -1\\ 
2& 0 & -2\\
1& 0 & -1\\
\end{pmatrix}\text{ and }
\begin{pmatrix}
3& 0 & -3\\ 
10& 0 & -10\\
3& 0 & -3\\
\end{pmatrix}\text{respectively.}
$$

The idea behind Convolutional Neural Nets is to treat these weights as learnable parameters - so to define the kernel as 

$$
\begin{pmatrix}
w_1& w_2 & w_3\\ 
w_4& w_5 & w_6\\
w_7& w_8 & w_9\\
\end{pmatrix}.
$$ 

Then backpropogation can learn the best filters to solve the problem at hand.

__Technical Note:__ In mathematics, what we describe above is referred to as *cross-correlation*. A convolution would require the kernal to first be mirrored on both axes before being applied to the image. However, machine learning works with the convention that a convolution skips this step.


##Padding

Convolutions typically have issues at the edge of images - forcing you to reduce the size of the result of the convolution. Padding just means that you surround your image with zeroes so that you can apply the convolution to every pixel.

Terminology:

* Valid convolution: don't use any padding, the result of your convolution is smaller than your original image
* Same convolution: Pad the input so that the output is the same size as the original input.

Convolution-filter sizes are typically odd, since then the calculation for how many pixels to pad with for same-convolutions is simple.

##Strided convolutions

Instead of applying the convolution to every single pixel, you can apply it to every second pixel, or every third pixel.


Calculation:

if you have

* An $n \times n$ image, 
* an $f \times f$ filter, 
* using padding $p$ and 
* stride-length $s$, 

then the output size of each dimension is given by $\frac{n+2p-f}{s}+1$.


##Convolutions over volume (3-dimensional images)

![Convolution over a 6x6 RGB image. Note the result does not have a third dimension.](Images/3D-CNN2.PNG)


So, applying a single convolution to an image returns a $k\times k \times 1$ image for some k. But if we apply say $n_c$ different convolutions to an image, we can stack these convolutions on top of each other to create a $k \times k \times n_c$ object or *volume*. The last dimension of the volume is referred to as the *depth* (Somewhat confusingly) or the number of channels of the volume. 

##One Layer of a Convolutional Network

![From https://www.coursera.org/learn/convolutional-neural-networks/lecture/nsiuW/one-layer-of-a-convolutional-network](Images/CNN-Example-of_a_layer.PNG)

Note how similar the setup is to a normal Feed-Forward Neural Network. A single Neuron in a CNN consists of some activation function of a convolution plus a bias term.

Say a single layer of your CNN consists of 10 filters of size $3\times 3 \times 3$. How many parameters would you have?

* Each convolution consists of $3 \cdot 3 \cdot 3 = 27$ weights
* Each convolutional network has it's own bias term
* Therefore the total number of weights in this layer is $(27 +1 ) \cdot 10 = 280$ parameters.
* Note the number of parameters is independent of the size of the previous layer. So the number of parameters to be estimated is not related to the number of pixels in the image.

##Summary of notation

If layer *l* is a convolution layer:

* $f^{[l]}$ = filter size. Note the number of channels in the filter should match the number of channels of the input, so $f^{[l]} \times f^{[l]} \times n_c^{[l-1]}$. 
* $p^{[l]}$ = padding size
* $s^{[l]}$ = stride size
* $n_c^{[l]}$ = number of filters
* Input = $n_H^{[l-1]} \times n_W^{[l-1]} n_c^{[l-1]}$ with subscripts representing height, width and channel respectively.
* Then Output: $n_H^{[l]} \times n_W^{[l]} \times n_c^{[l]}$ where
    - $n^{[l]} = \lfloor \frac{n^{[l-1]} +2p^{[l]}-f^{[l]}}{[l]}{2} + 1 \rfloor$ where the subscripts should be filled to denote width or height.
* Activation dimensions match the Output. so $a^{[l]} = n_H^{[l]} \times n_W^{[l]} \times n_c^{[l]}$
* If you're using batch-gradient descent, then activations will be stored with an extra dimension $A^{[l]} = m \times n_H^{[l]} \times n_W^{[l]} \times n_c^{[l]}$
* Weights: $f^{[l]} \times f^{[l]} \times n_c^{[l-1]} \times n_c^{[l]}$
* bias: $n_c^{[l]}$ which would be stored with dimensions $(1,1,1,n_c^{[l]})$

##Pooling

![Max-pooling with *stride-size*=2 and *filter-size*=2.](Images/max_pooling.PNG)

Pooling is a resolution-reduction layer applied within a Neural Network. There are 2 main types of which the most commonly used is max-pooling.

* _Max-pooling_ applies a window function to each channel and outputs the maximum value of each pixel in that channel
* _Average-pooling_ outputs the mean-value of each pixel in that channel.

The intuition of max-pooling is that, when a convolution is applied, you're trying to detect a feature in that area. Thus the maximum value within a window contains most of the relevant information for that region. Average-pooling is only really used to collapse your representation to a $1\times 1\times n_c$.

Note that there are no parameters used in pooling! Only hyperparameters for the stride and filter sizes used for the layer. typical values are $f=2, s=2$ which reduces the height and width of the representation by a factor of 2, or $f=3, s=2$. It is possible to use padding as well, although this is rarely used (one exception later in this course).

Note the change in size of pooling:

* Output size: $\lfloor \frac{n - f}{s} +1\rfloor$ for height and width
* Number of channels remains the same since the 2D-Convolution is applied to each channel 1 at a time.

##Example of Architecture
![NN Architecture example](Images/Example of Neural Net Architecture.PNG)
Imagine you're fitting a NN to the MNIST dataset. I will just list out the steps in the above image here. You begin with a $32 \times 32$ RBG-image. Then:
1. 
  - Apply a Convolution layer with filter-size = 5 and stride-length=1 and 6 channels
  - Apply max-pooling with filter-size=2, stride-length = 2.
  - Note typically a pooling layer is still counted as the same layer as it's input (since there are no parameters I guess)
2.
  - Apply a Convolution layer with filter-size = 5 and stride-length=1 and 16 channels
  - Apply max-pooling with filter-size=2, stride-length = 2.
  - The resultant representation is now flattened out into a $400 \times 1$ representation
3.
  -Apply a fully-connected layer of size 120.
4. 
  - Apply a fully-connected layer of size 84
5.
  - Apply either a softmax-layer of size 10

There are several things to note in this example:
    
* This pattern of applying convolutions then a fully-connected layers is typical.
* The size of the width and height of the representation typically decrease as you go through the layers
* The number of channels typically increases as you go through the layers
* The dense-layers typically use far more weights than the convolutional layers.

```{r, echo=FALSE}
params<-rlist::list.stack(list(
list("Input","(32,32,3)",3072,0),
list("Conv1(f=5,s=1)","(28,28,8)",6272,608),
list("POOL1","(14,14,8)",1568,0),
list("Input","(10,10,16)",1600,3216),
list("POOL1","(5,5,16)",400,0),
list("FC3","(120,1)",120,48120),
list("FC4","(84,1)",84,10164),
list("Softmax","(10,1)",10,850)
))

names(params)<-c('Layer','Activation shape','Activation Size','# parameters')

knitr::kable(params)%>%kable_styling(bootstrap_options = c("striped", "hover"))
```

##Why Convolutions?

Main advantages:

1. Parameter Sharing - A feature detector (such as a vertical edge detector) that's useful in one part of an image is probably useful in another part of an image.

2. Sparcity of connections - In each layer, each output value depends on only a small number of inputs.

For parameter sharing, if we're looking for a cat in an image, we'd want to use the same search regardless of where in the image we're looking. This means CNNs are robust to *Translation Invariance* (meaning a cat shifted a few pixels to the right is still a cat)

Sparcity of connections means we don't have as many parameters we need to learn.

Andrew Ng doesn't make the point here, but its made in other books, that Convolutional layers have the idea that the relationships between nearby pixels is probably more important than the relationships of distant pixels. Thus a CNN starts off looking in small regions and, as the representation decreases in height and width, information from further away in the image gets interacted with.




